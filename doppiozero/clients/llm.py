"""Lightweight LLM client wrapper.

Provides two functions used by the agent:
- generate(prompt, model=None): returns a string completion
- embed(text, model=None): returns a list[float]

If OPENAI_API_KEY is present in the environment, the client will call the
OpenAI REST API. Otherwise it falls back to deterministic local stubs so the
code is testable without network access.
"""

from typing import List, Optional
import os
import json
import urllib.request
import ssl


class LLMClient:
    """Lightweight LLM client wrapper.

    Methods:
    - generate(prompt, model=None): returns a string completion
    - embed(text, model=None): returns a list[float]

    If OPENAI_API_KEY is present in the environment (or provided), the client
    will call the OpenAI REST API. Otherwise it falls back to deterministic
    local stubs so the code is testable without network access.
    """

    def __init__(self, api_key: Optional[str] = None, api_base: Optional[str] = None):
        """Create an LLM client using optional API credentials and base URL.

        Args:
            api_key : Optional API key; falls back to OPENAI_API_KEY env var.
            api_base : Optional API base URL; falls back to OPENAI_API_BASE env var.

        Returns:
            None

        """
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        self.api_base = api_base or os.environ.get("OPENAI_API_BASE", "https://api.openai.com")

    def _call_openai_api(self, path: str, payload: dict) -> dict:
        """Perform an HTTP POST to the OpenAI-compatible REST endpoint.

        Args:
            path : The API path (e.g. '/v1/chat/completions').
            payload : The JSON-serializable payload to send.

        Returns:
            The parsed JSON response as a dictionary.

        """
        url = self.api_base.rstrip("/") + path
        data = json.dumps(payload).encode("utf-8")
        header_dc = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
        }
        req = urllib.request.Request(url, data=data, headers=header_dc)
        ctx = ssl.create_default_context()
        with urllib.request.urlopen(req, context=ctx) as resp:
            return json.loads(resp.read().decode("utf-8"))

    def generate(self, prompt: str, model: Optional[str] = None, max_tokens: int = 1024) -> str:
        """Return a string generated by the configured LLM or a deterministic stub.

        Args:
            prompt : The prompt text to send to the LLM.
            model : Optional model name to override the default.
            max_tokens : Maximum tokens to request from the API.

        Returns:
            A generated text string from the LLM or a simulated summary on fallback.

        """
        if self.api_key:
            model_name = model or os.environ.get("SUMMARIZATION_MODEL", "gpt-3.5-turbo")
            payload_dc = {
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
            }
            try:
                resp = self._call_openai_api("/v1/chat/completions", payload_dc)
                choice_ls = resp.get("choices") or []
                if choice_ls:
                    return choice_ls[0].get("message", {}).get("content", "")
                return ""
            except Exception as e:
                raise RuntimeError(f"LLM request failed: {e}")
        # Fallback stub
        excerpt = prompt[:400]
        return f"[SIMULATED SUMMARY]\n\n{excerpt}\n..."

    def embed(self, text: str, model: Optional[str] = None) -> List[float]:
        """Return an embedding vector (list of floats) for the given text.

        Args:
            text : The input text to embed.
            model : Optional model name to override the default embedding model.

        Returns:
            A list of floats representing the embedding vector.

        """
        if self.api_key:
            model_name = model or os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
            payload_dc = {"model": model_name, "input": text}
            try:
                resp = self._call_openai_api("/v1/embeddings", payload_dc)
                data_ls = resp.get("data") or []
                if data_ls:
                    return data_ls[0].get("embedding", [])
                return []
            except Exception as e:
                raise RuntimeError(f"Embeddings request failed: {e}")
        # Fallback deterministic pseudo-embedding
        v_ls = [0.0] * 128
        h = 0
        for ch in text:
            h = (h * 31 + ord(ch)) & 0xFFFFFFFF
        for i in range(len(v_ls)):
            v_ls[i] = ((h >> (i % 32)) & 0xFF) / 255.0
        return v_ls


# Backward-compat convenience: a default client instance can be used by callers
# that prefer not to instantiate explicitly. We will update callers to use the
# class directly, but keep this here for optional compatibility.
llm_client = LLMClient()

# Note: `llm_client` is the canonical default client instance.
