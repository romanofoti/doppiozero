"""Lightweight LLM client wrapper.

Provides two functions used by the agent:
- generate(prompt, model=None): returns a string completion
- embed(text, model=None): returns a list[float]

If OPENAI_API_KEY is present in the environment, the client will call the
OpenAI REST API. Otherwise it falls back to deterministic local stubs so the
code is testable without network access.
"""

from typing import List, Optional
import os
import json
import urllib.request
import ssl


class LLMClient:
    """Lightweight LLM client wrapper.

    Methods:
    - generate(prompt, model=None): returns a string completion
    - embed(text, model=None): returns a list[float]

    If OPENAI_API_KEY is present in the environment (or provided), the client
    will call the OpenAI REST API. Otherwise it falls back to deterministic
    local stubs so the code is testable without network access.
    """

    def __init__(self, api_key: Optional[str] = None, api_base: Optional[str] = None):
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        self.api_base = api_base or os.environ.get("OPENAI_API_BASE", "https://api.openai.com")

    def _call_openai_api(self, path: str, payload: dict) -> dict:
        url = self.api_base.rstrip("/") + path
        data = json.dumps(payload).encode("utf-8")
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        req = urllib.request.Request(url, data=data, headers=headers)
        ctx = ssl.create_default_context()
        with urllib.request.urlopen(req, context=ctx) as resp:
            return json.loads(resp.read().decode("utf-8"))

    def generate(self, prompt: str, model: Optional[str] = None, max_tokens: int = 1024) -> str:
        """Return a string generated by the configured LLM or a deterministic stub."""
        if self.api_key:
            model_name = model or os.environ.get("SUMMARIZATION_MODEL", "gpt-3.5-turbo")
            payload_dc = {
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
            }
            try:
                resp = self._call_openai_api("/v1/chat/completions", payload_dc)
                choice_ls = resp.get("choices") or []
                if choice_ls:
                    return choice_ls[0].get("message", {}).get("content", "")
                return ""
            except Exception as e:
                raise RuntimeError(f"LLM request failed: {e}")
        # Fallback stub
        excerpt = prompt[:400]
        return f"[SIMULATED SUMMARY]\n\n{excerpt}\n..."

    def embed(self, text: str, model: Optional[str] = None) -> List[float]:
        """Return an embedding vector (list of floats) for the given text."""
        if self.api_key:
            model_name = model or os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
            payload_dc = {"model": model_name, "input": text}
            try:
                resp = self._call_openai_api("/v1/embeddings", payload_dc)
                data_ls = resp.get("data") or []
                if data_ls:
                    return data_ls[0].get("embedding", [])
                return []
            except Exception as e:
                raise RuntimeError(f"Embeddings request failed: {e}")
        # Fallback deterministic pseudo-embedding
        v_ls = [0.0] * 128
        h = 0
        for ch in text:
            h = (h * 31 + ord(ch)) & 0xFFFFFFFF
        for i in range(len(v_ls)):
            v_ls[i] = ((h >> (i % 32)) & 0xFF) / 255.0
        return v_ls


# Backward-compat convenience: a default client instance can be used by callers
# that prefer not to instantiate explicitly. We will update callers to use the
# class directly, but keep this here for optional compatibility.
llm_client = LLMClient()

# Note: `llm_client` is the canonical default client instance.
