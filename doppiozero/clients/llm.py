"""Lightweight LLM client wrapper.

Provides two functions used by the agent:
- generate(prompt, model=None): returns a string completion
- embed(text, model=None): returns a list[float]

If OPENAI_API_KEY is present in the environment, the client will call the
OpenAI REST API. Otherwise it falls back to deterministic local stubs so the
code is testable without network access.
"""

import json
import os
import yaml

from typing import Any, Dict, List, Optional

from openai import AzureOpenAI

from ..utils.utils import get_logger

logger = get_logger(__name__)


class LLMClient:
    """
    Lightweight LLM client wrapper providing text generation and embeddings.

    Parameters
    ----------
    api_key : Optional[str]
        API key for the OpenAI-compatible endpoint. Falls back to the
        OPENAI_API_KEY environment variable when omitted.
    api_url : Optional[str]
        Specific API URL for the request. Falls back to OPENAI_URL environment variable.
    verbose : bool
        Whether to print verbose logging information.

    Attributes
    ----------
    api_key : Optional[str]
        The API key used for requests.
    api_url : str
        The specific API URL used for requests.
    verbose : bool
        Whether to print verbose logging information.

    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        api_url: Optional[str] = None,
        verbose: bool = False,
    ):
        """Create an LLM client using optional API credentials and base URL.

        Args:
            api_key : Optional API key; falls back to OPENAI_API_KEY env var.
            api_base : Optional API base URL; falls back to OPENAI_API_BASE env var.
            api_url : Optional API URL; falls back to OPENAI_URL env var.

        Returns:
            None

        """
        self.api_key = api_key or os.environ.get("GPT_5_MINI_KEY")
        self.api_url = api_url or os.environ.get("OPENAI_URL")
        self.verbose = verbose

    def _process_raw_output(self, result_dc) -> Dict[str, Any]:
        """Parse the LLM output and extract only the 'determination' dictionary.

        Args:
            result_dc (dict): The result dictionary from the LLM.

        Returns:
            dict: A dictionary containing the 'determination' fields if present.

        Note:
            - If the output is a valid JSON object with a 'determination' key, it returns that.
            - If the output is malformed, it returns an empty dict.

        """

        raw_output = result_dc["choices"][0]["message"]["content"]
        yaml_str = raw_output.split("```yaml")[-1].split("```")[0].strip()
        response_dc = yaml.safe_load(yaml_str)
        return response_dc if isinstance(response_dc, dict) else {}

    def _call_openai_api(self, prompt: dict) -> dict:
        """Perform an HTTP POST to the OpenAI-compatible REST endpoint.

        Args:
            prompt (dict): The prompt to send to the API.

        Returns:
            The parsed response as a dictionary.

        """
        response_dc = {}

        client = AzureOpenAI(
            api_version="2024-12-01-preview", azure_endpoint=self.api_url, api_key=self.api_key
        )

        if self.verbose:
            logger.info("-------------------------------------")
            logger.info("Submitting the request to the LLM...")
            logger.info("-------------------------------------")

        response_dc = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=int(os.environ.get("MAX_TOKENS", 1024)),
            model=os.environ.get("MODEL", "gpt-5-mini"),
        ).dict()

        return response_dc

    def generate(self, prompt: str, model: Optional[str] = None, max_tokens: int = 1024) -> str:
        """Return a string generated by the configured LLM or a deterministic stub.

        Args:
            prompt : The prompt text to send to the LLM.
            model : Optional model name to override the default.
            max_tokens : Maximum tokens to request from the API.

        Returns:
            A generated text string from the LLM or a simulated summary on fallback.

        """
        result_dc = response_dc = {}
        if self.api_key:
            try:
                response_dc = self._call_openai_api(prompt)
                result_dc = self._process_raw_output(response_dc)
            except Exception as e:
                logger.info("-------------------------------------")
                raise RuntimeError(f"LLM request failed: {e}")
                logger.info("-------------------------------------")

        if self.verbose:
            logger.info("**********************************")
            logger.info("Returning the following output:")
            if result_dc:
                logger.info(json.dumps(result_dc, indent=2))
            else:
                result_dc = {"fallback": "Dummy output"}
                logger.info("No valid output received.")
            logger.info("**********************************")

        return result_dc, response_dc

    def embed(self, text: str, model: Optional[str] = None) -> List[float]:
        """Return an embedding vector (list of floats) for the given text.

        Args:
            text : The input text to embed.
            model : Optional model name to override the default embedding model.

        Returns:
            A list of floats representing the embedding vector.

        """
        if self.api_key:
            model_name = model or os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
            payload_dc = {"model": model_name, "input": text}
            try:
                resp = self._call_openai_api("/v1/embeddings", payload_dc)
                data_ls = resp.get("data") or []
                if data_ls:
                    return data_ls[0].get("embedding", [])
                return []
            except Exception as e:
                raise RuntimeError(f"Embeddings request failed: {e}")
        # Fallback deterministic pseudo-embedding
        v_ls = [0.0] * 128
        h = 0
        for ch in text:
            h = (h * 31 + ord(ch)) & 0xFFFFFFFF
        for i in range(len(v_ls)):
            v_ls[i] = ((h >> (i % 32)) & 0xFF) / 255.0
        return v_ls


# Backward-compat convenience: a default client instance can be used by callers
# that prefer not to instantiate explicitly. We will update callers to use the
# class directly, but keep this here for optional compatibility.
llm_client = LLMClient()

# Note: `llm_client` is the canonical default client instance.
