"""Lightweight LLM client wrapper.

Provides two functions used by the agent:
- generate(prompt, model=None): returns a string completion
- embed(text, model=None): returns a list[float]

If OPENAI_API_KEY is present in the environment, the client will call the
OpenAI REST API. Otherwise it falls back to deterministic local stubs so the
code is testable without network access.
"""

from typing import List, Optional
import os
import json
import urllib.request
import ssl


OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_API_BASE = os.environ.get("OPENAI_API_BASE", "https://api.openai.com")


def _call_openai_api(path: str, payload: dict) -> dict:
    url = OPENAI_API_BASE.rstrip("/") + path
    data = json.dumps(payload).encode("utf-8")
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {OPENAI_API_KEY}"}
    req = urllib.request.Request(url, data=data, headers=headers)
    ctx = ssl.create_default_context()
    with urllib.request.urlopen(req, context=ctx) as resp:
        return json.loads(resp.read().decode("utf-8"))


def generate(prompt: str, model: Optional[str] = None, max_tokens: int = 1024) -> str:
    """Return a string generated by the configured LLM or a deterministic stub.

    Args:
        prompt: the prompt text to send to the model
        model: optional model string
    """
    if OPENAI_API_KEY:
        model_name = model or os.environ.get("SUMMARIZATION_MODEL", "gpt-3.5-turbo")
        payload = {
            "model": model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
        }
        try:
            resp = _call_openai_api("/v1/chat/completions", payload)
            # Extract content
            choices = resp.get("choices") or []
            if choices:
                return choices[0].get("message", {}).get("content", "")
            return ""
        except Exception as e:
            raise RuntimeError(f"LLM request failed: {e}")
    # Fallback stub
    # Keep deterministic output for tests
    excerpt = prompt[:400]
    return f"[SIMULATED SUMMARY]\n\n{excerpt}\n..."


def embed(text: str, model: Optional[str] = None) -> List[float]:
    """Return an embedding vector (list of floats) for the given text.

    Uses OpenAI embeddings when API key is present, otherwise returns a
    deterministic pseudo-embedding vector.
    """
    if OPENAI_API_KEY:
        model_name = model or os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
        payload = {"model": model_name, "input": text}
        try:
            resp = _call_openai_api("/v1/embeddings", payload)
            data = resp.get("data") or []
            if data:
                return data[0].get("embedding", [])
            return []
        except Exception as e:
            raise RuntimeError(f"Embeddings request failed: {e}")
    # Fallback deterministic pseudo-embedding
    # Simple hash-based embedding into 128-d vector
    v = [0.0] * 128
    h = 0
    for ch in text:
        h = (h * 31 + ord(ch)) & 0xFFFFFFFF
    # Spread h into vector deterministically
    for i in range(len(v)):
        v[i] = ((h >> (i % 32)) & 0xFF) / 255.0
    return v
